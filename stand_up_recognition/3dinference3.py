import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
import os, time,cv2,torch
import numpy as np
from torchvision.models.video import r3d_18
import torch.nn as nn
from collections import defaultdict
class VideoTransform:
    """å°æ•´å€‹ clip (æ‰€æœ‰å¹€) åŒæ­¥åšæ•¸æ“šå¢å¼·"""
    def __init__(self, resize=(224, 224), use_tubemask=True):
        self.resize = resize
        self.train_transform = transforms.Compose([
            transforms.Resize(resize),
            transforms.RandomResizedCrop(resize, scale=(0.8, 1.0)),
            transforms.RandomHorizontalFlip(),
            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),
            transforms.RandomRotation(10),
            transforms.ToTensor(),
        ])
        self.test_transform = transforms.Compose([
            transforms.Resize(resize),
            transforms.ToTensor(),
        ])
        # Random Erasing
        self.random_erasing = transforms.RandomErasing(
            p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=0
        )
        self.tube_masking = TubeMasking(mask_ratio=0.15, mode="zero")
        self.use_tubemask = use_tubemask

    def __call__(self, clip, mode="train"):
        seed = np.random.randint(99999)
        transformed_clip = []
        transform = self.train_transform if mode == "train" else self.test_transform
        for img in clip:
            random.seed(seed)
            transformed_clip.append(transform(img))
        #   # (T=10, C=3, H=224, W=112)
        clip_tensor = torch.stack(transformed_clip)  # (T, C, H, W)

        if mode == "train":
            # Random Erasing (é€å¹€)
            for t in range(clip_tensor.shape[0]):
                clip_tensor[t] = self.random_erasing(clip_tensor[t])

            # TubeMasking (æ•´æ®µ clip)
            if self.use_tubemask:
                clip_tensor = clip_tensor.permute(1, 0, 2, 3)  # (C,T,H,W)
                clip_tensor = self.tube_masking(clip_tensor)
                clip_tensor = clip_tensor.permute(1, 0, 2, 3)  # (T,C,H,W)

        return clip_tensor

# ---- Temporal Jittering ----
def temporal_jitter(img_list, num_frames,mode):
    """éš¨æ©Ÿæ‰“äº‚æ™‚é–“å–æ¨£ï¼Œæ¨¡æ“¬ä¸åŒå‹•ä½œå¿«æ…¢"""
    if len(img_list) < num_frames:
        img_list.extend([img_list[-1]] * (num_frames - len(img_list)))
    else:
        if mode == 'train':
            indices = sorted(random.sample(range(len(img_list)), num_frames))
        else:
            #uniform_sample
            indices = np.linspace(0, len(img_list) - 1, num_frames).astype(int)
        img_list = [img_list[i] for i in indices]
    return img_list
# ç¢ºä¿ä½ çš„ VideoTransform é¡å’Œ Temporal Jittering å‡½æ•¸è¢«å°å…¥æˆ–å®šç¾©åœ¨æ­¤è…³æœ¬ä¸­
# é€™è£¡æˆ‘ç›´æ¥å°‡å®ƒå€‘è²¼å‡ºä¾†ï¼Œä»¥ç¢ºä¿å®Œæ•´æ€§
# --- VideoTransform Class ---
class VideoTransform:
    def __init__(self, resize=(224, 112), use_tubemask=False):  # æ³¨æ„ï¼Œæ¨æ–·æ™‚ä¸ä½¿ç”¨ TubeMasking
        self.resize = resize
        self.transform = transforms.Compose([
            transforms.Resize(resize),
            transforms.ToTensor(),
        ])

    def __call__(self, clip):
        transformed_clip = [self.transform(img) for img in clip]
        clip_tensor = torch.stack(transformed_clip)  # (T, C, H, W)
        return clip_tensor


# --- Temporal Jittering Function ---
def temporal_jitter(img_list, num_frames, mode):
    if len(img_list) < num_frames:
        # ä¸è¶³å°±è£œæœ€å¾Œä¸€å¹€
        img_list.extend([img_list[-1]] * (num_frames - len(img_list)))
    else:
        # æ¨æ–·æ™‚ä½¿ç”¨å‡å‹»æ¡æ¨£
        indices = np.linspace(0, len(img_list) - 1, num_frames).astype(int)
        img_list = [img_list[i] for i in indices]
    return img_list


# --- æ¨æ–·ç›¸é—œé…ç½® ---
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
class_names = ['always_sit', 'movement', 'stand_up']
num_frames = 10  # ç¢ºä¿èˆ‡è¨“ç·´æ™‚çš„ frames_per_clip åƒæ•¸ä¸€è‡´


def load_model(model_name, num_classes):
    """
    åŠ è¼‰æ¨¡å‹ä¸¦è¼‰å…¥æ¬Šé‡
    """
    if model_name == 'r3d18':
        model = r3d_18(pretrained=False)
        model.fc = nn.Linear(model.fc.in_features, num_classes)
        model.load_state_dict(torch.load('./r3d18_action_3class_frozen.pth', map_location=device))
    else:
        raise ValueError("Unsupported model name")

    model.to(device)
    model.eval()
    return model


def inference(video_path, model, class_names):
    """
    å°å–®å€‹è¦–é »è·¯å¾‘é€²è¡Œæ¨æ–·
    """
    # 1. è®€å–åœ–ç‰‡åºåˆ—æˆ–è¦–é »æ–‡ä»¶
    img_list = []
    if os.path.isdir(video_path):
        all_img_name = sorted([f for f in os.listdir(video_path) if f.endswith('.jpg')])
        for img_name in all_img_name:
            full_img_path = os.path.join(video_path, img_name)
            img_pil = Image.open(full_img_path).convert("RGB")
            img_list.append(img_pil)
    else:  # å‡è¨­æ˜¯è¦–é »æ–‡ä»¶
        cap = cv2.VideoCapture(video_path)
        while True:
            ret, cur_frame = cap.read()
            if not ret:
                break
            cur_frame = cv2.cvtColor(cur_frame, cv2.COLOR_BGR2RGB)
            img_pil = Image.fromarray(cur_frame)
            img_list.append(img_pil)

    # 2. é€²è¡Œæ™‚é–“æŠ–å‹• (Temporal Jittering)ï¼Œä½¿ç”¨èˆ‡è¨“ç·´æ™‚ç›¸åŒçš„æ¨¡å¼ 'test'
    img_list = temporal_jitter(img_list, num_frames, mode='test')

    # 3. é€²è¡Œè¦–é »æ•¸æ“šè½‰æ› (VideoTransform)ï¼Œä½¿ç”¨èˆ‡æ¸¬è©¦æ™‚ç›¸åŒçš„é‚è¼¯
    video_transform = VideoTransform(resize=(224, 112))
    clip_tensor = video_transform(img_list)  # (T, C, H, W)

    # 4. è½‰æ› Tensor æ ¼å¼ä»¥ç¬¦åˆæ¨¡å‹è¼¸å…¥ (C, T, H, W)
    clip_tensor = clip_tensor.permute((1, 0, 2, 3))

    # 5. æ“´å±•ç¶­åº¦ç‚º (B=1, C, T, H, W)
    inputs = clip_tensor.unsqueeze(0).to(device)

    # 6. æ¨æ–·
    with torch.no_grad():
        # æ¨™æº–åŒ–ï¼ˆèˆ‡è¨“ç·´æ™‚çš„ DataLoader é‚è¼¯ä¸€è‡´ï¼‰
        inputs_m, inputs_s = inputs.mean(), inputs.std()
        inputs = (inputs - inputs_m) / inputs_s

        outputs = model(inputs)
        probs = F.softmax(outputs, dim=1)
        pred = torch.argmax(probs, dim=1).item()

    return class_names[pred], probs.cpu().numpy()[0]


def evaluate_all_per_class(test_base_path, model, class_names):
    """
    è©•ä¼°æ¸¬è©¦é›†ä¸Šæ‰€æœ‰é¡åˆ¥çš„æº–ç¢ºç‡ï¼Œä¸¦é¡¯ç¤ºæ¯å€‹é¡åˆ¥çš„çµæœã€‚

    Args:
        test_base_path (str): æ¸¬è©¦é›†æ ¹ç›®éŒ„ï¼Œä¾‹å¦‚ '/home/zonekey/project/3d_train_test/test'
        model: å·²åŠ è¼‰çš„æ¨¡å‹
        class_names: é¡åˆ¥åç¨±åˆ—è¡¨
    """
    # ä½¿ç”¨ defaultdict æ–¹ä¾¿åœ°è¿½è¹¤æ¯å€‹é¡åˆ¥çš„è¨ˆæ•¸
    class_correct = defaultdict(int)
    class_total = defaultdict(int)

    # éæ­·æ¸¬è©¦é›†æ ¹ç›®éŒ„ä¸‹çš„æ‰€æœ‰é¡åˆ¥ç›®éŒ„
    for class_name in class_names:
        class_dir = os.path.join(test_base_path, class_name)

        # éæ­·è©²é¡åˆ¥ç›®éŒ„ä¸‹çš„æ‰€æœ‰è¦–é »æ¨£æœ¬
        sample_path_list = os.listdir(class_dir)

        for video_dir in sample_path_list:
            # ç²å–å–®å€‹æ¨£æœ¬çš„å®Œæ•´è·¯å¾‘
            full_video_path = os.path.join(class_dir, video_dir)

            # é€²è¡Œæ¨æ–·
            try:
                pred_label_name, _ = inference(full_video_path, model, class_names)

                # å–å¾—çœŸå¯¦æ¨™ç±¤
                true_label_name = class_name

                # æ›´æ–°è¨ˆæ•¸
                class_total[true_label_name] += 1
                if pred_label_name == true_label_name:
                    class_correct[true_label_name] += 1
            except Exception as e:
                print(f"Skipping {full_video_path} due to error: {e}")

    print("\n--- ğŸ“Š é¡åˆ¥æº–ç¢ºç‡ (Per-Class Accuracy) ---")

    # è¨ˆç®—ä¸¦æ‰“å°æ¯å€‹é¡åˆ¥çš„æº–ç¢ºç‡
    total_correct = 0
    total_count = 0
    for class_name in class_names:
        correct = class_correct[class_name]
        total = class_total[class_name]
        accuracy = correct / total if total > 0 else 0
        print(f"  âœ… {class_name}: {accuracy:.4f} ({correct}/{total})")

        total_correct += correct
        total_count += total

    # è¨ˆç®—ä¸¦æ‰“å°ç¸½é«”æº–ç¢ºç‡
    overall_accuracy = total_correct / total_count if total_count > 0 else 0
    print(f"\n--- âœ… ç¸½é«”æº–ç¢ºç‡ (Overall Accuracy): {overall_accuracy:.4f} ---")


if __name__ == '__main__':
    model = load_model('r3d18',num_classes=3)

    video_path = "/home/zonekey/project/3d_train_test/test/stand_up/t188.07-p0"
    label, probs = inference(video_path, model, class_names)

    print("Prediction:", label)
    print("Probabilities:", dict(zip(class_names, probs)))

    # ä½¿ç”¨æ­£ç¢ºçš„ evaluate_all å‡½æ•¸ä¾†è©•ä¼°æ•´å€‹æ¸¬è©¦é›†
    test_path = '/home/zonekey/project/3d_train_test/test'
    #evaluate_all_per_class(test_path, model, class_names)